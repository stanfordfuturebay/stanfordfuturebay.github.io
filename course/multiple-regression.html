<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 Multiple regression | Shaping the Future of the Bay Area: Intro to Urban Data Analytics in R</title>
  <meta name="description" content="5.2 Multiple regression | Shaping the Future of the Bay Area: Intro to Urban Data Analytics in R" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 Multiple regression | Shaping the Future of the Bay Area: Intro to Urban Data Analytics in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 Multiple regression | Shaping the Future of the Bay Area: Intro to Urban Data Analytics in R" />
  
  
  

<meta name="author" content="Stanford Future Bay Initiative" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="autocorrelation.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/proj4-2.6.2/proj4.min.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.4.1/leaflet.js"></script>
<script src="libs/leaflet-providers-1.9.0/leaflet-providers_1.9.0.js"></script>
<script src="libs/leaflet-providers-plugin-2.0.4.1/leaflet-providers-plugin.js"></script>
<link href="libs/HomeButton-0.0.1/home-button.css" rel="stylesheet" />
<script src="libs/HomeButton-0.0.1/home-button.js"></script>
<script src="libs/HomeButton-0.0.1/easy-button-src.min.js"></script>
<script src="libs/clipboard-0.0.1/setClipboardText.js"></script>
<link href="libs/mapviewCSS-0.0.1/mapview-popup.css" rel="stylesheet" />
<link href="libs/mapviewCSS-0.0.1/mapview.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://bay.stanford.edu">Stanford Future Bay Initiative</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="software-setup.html"><a href="software-setup.html"><i class="fa fa-check"></i><b>1.1</b> Software Setup</a></li>
<li class="chapter" data-level="1.2" data-path="rstudio-interface.html"><a href="rstudio-interface.html"><i class="fa fa-check"></i><b>1.2</b> RStudio Interface</a></li>
<li class="chapter" data-level="1.3" data-path="r-markdown-files.html"><a href="r-markdown-files.html"><i class="fa fa-check"></i><b>1.3</b> R Markdown Files</a></li>
<li class="chapter" data-level="1.4" data-path="github.html"><a href="github.html"><i class="fa fa-check"></i><b>1.4</b> GitHub</a></li>
<li class="chapter" data-level="1.5" data-path="reading-and-saving-files.html"><a href="reading-and-saving-files.html"><i class="fa fa-check"></i><b>1.5</b> Reading and saving files</a></li>
<li class="chapter" data-level="1.6" data-path="loops.html"><a href="loops.html"><i class="fa fa-check"></i><b>1.6</b> Loops</a></li>
<li class="chapter" data-level="1.7" data-path="manipulating-data.html"><a href="manipulating-data.html"><i class="fa fa-check"></i><b>1.7</b> Manipulating data</a></li>
<li class="chapter" data-level="1.8" data-path="plots.html"><a href="plots.html"><i class="fa fa-check"></i><b>1.8</b> Plots</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="populations.html"><a href="populations.html"><i class="fa fa-check"></i><b>2</b> Populations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="geospatial-data.html"><a href="geospatial-data.html"><i class="fa fa-check"></i><b>2.1</b> Geospatial data</a></li>
<li class="chapter" data-level="2.2" data-path="acs-data.html"><a href="acs-data.html"><i class="fa fa-check"></i><b>2.2</b> ACS data</a></li>
<li class="chapter" data-level="2.3" data-path="decennial-data.html"><a href="decennial-data.html"><i class="fa fa-check"></i><b>2.3</b> Decennial Data</a></li>
<li class="chapter" data-level="2.4" data-path="spatial-subsets.html"><a href="spatial-subsets.html"><i class="fa fa-check"></i><b>2.4</b> Spatial subsets</a></li>
<li class="chapter" data-level="2.5" data-path="migration.html"><a href="migration.html"><i class="fa fa-check"></i><b>2.5</b> Migration</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="surveys.html"><a href="surveys.html"><i class="fa fa-check"></i><b>3</b> Surveys</a>
<ul>
<li class="chapter" data-level="3.1" data-path="microdata.html"><a href="microdata.html"><i class="fa fa-check"></i><b>3.1</b> Microdata</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>4</b> Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="equity-analysis.html"><a href="equity-analysis.html"><i class="fa fa-check"></i><b>4.1</b> Equity analysis</a></li>
<li class="chapter" data-level="4.2" data-path="probability-distributions.html"><a href="probability-distributions.html"><i class="fa fa-check"></i><b>4.2</b> Probability distributions</a></li>
<li class="chapter" data-level="4.3" data-path="monte-carlo-simulations.html"><a href="monte-carlo-simulations.html"><i class="fa fa-check"></i><b>4.3</b> Monte Carlo simulations</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>5</b> Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>5.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="5.2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multiple regression</a></li>
<li class="chapter" data-level="5.3" data-path="autocorrelation.html"><a href="autocorrelation.html"><i class="fa fa-check"></i><b>5.3</b> Autocorrelation</a></li>
<li class="chapter" data-level="5.4" data-path="survey-regression.html"><a href="survey-regression.html"><i class="fa fa-check"></i><b>5.4</b> Survey regression</a></li>
<li class="chapter" data-level="5.5" data-path="training-and-testing-data.html"><a href="training-and-testing-data.html"><i class="fa fa-check"></i><b>5.5</b> Training and testing data</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Shaping the Future of the Bay Area: Intro to Urban Data Analytics in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-regression" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Multiple regression</h2>
<p>In principle, multiple regression merely adds additional independent variables to the same math we’ve done before, but since this causes the model to no longer be viewable in a two-dimensional scatter plot, it feels more substantively different than it should. If you boil down the difference to additional <code>mx</code> terms in <code>y = mx + b</code>, then this jump in dimensionality should feel much more manageable. Let’s go ahead and expand on the analyses we did of education and income at the census tract level. Let’s add one possibly explanatory variable: % of households that are White (-led).</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="multiple-regression.html#cb172-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb172-2"><a href="multiple-regression.html#cb172-2"></a><span class="kw">library</span>(censusapi)</span>
<span id="cb172-3"><a href="multiple-regression.html#cb172-3"></a><span class="kw">library</span>(sf)</span>
<span id="cb172-4"><a href="multiple-regression.html#cb172-4"></a><span class="kw">library</span>(mapview)</span>
<span id="cb172-5"><a href="multiple-regression.html#cb172-5"></a><span class="kw">library</span>(tigris)</span>
<span id="cb172-6"><a href="multiple-regression.html#cb172-6"></a></span>
<span id="cb172-7"><a href="multiple-regression.html#cb172-7"></a><span class="kw">Sys.setenv</span>(<span class="dt">CENSUS_KEY=</span><span class="st">&quot;c8aa67e4086b4b5ce3a8717f59faa9a28f611dab&quot;</span>)</span>
<span id="cb172-8"><a href="multiple-regression.html#cb172-8"></a></span>
<span id="cb172-9"><a href="multiple-regression.html#cb172-9"></a>acs_vars_<span class="dv">2019</span>_5yr &lt;-</span>
<span id="cb172-10"><a href="multiple-regression.html#cb172-10"></a><span class="st">  </span><span class="kw">listCensusMetadata</span>(</span>
<span id="cb172-11"><a href="multiple-regression.html#cb172-11"></a>    <span class="dt">name =</span> <span class="st">&quot;2019/acs/acs5&quot;</span>,</span>
<span id="cb172-12"><a href="multiple-regression.html#cb172-12"></a>    <span class="dt">type =</span> <span class="st">&quot;variables&quot;</span></span>
<span id="cb172-13"><a href="multiple-regression.html#cb172-13"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="multiple-regression.html#cb173-1"></a>bay_multiple_tract &lt;-<span class="st"> </span></span>
<span id="cb173-2"><a href="multiple-regression.html#cb173-2"></a><span class="st">  </span><span class="kw">getCensus</span>(</span>
<span id="cb173-3"><a href="multiple-regression.html#cb173-3"></a>    <span class="dt">name =</span> <span class="st">&quot;acs/acs5&quot;</span>,</span>
<span id="cb173-4"><a href="multiple-regression.html#cb173-4"></a>    <span class="dt">vintage =</span> <span class="dv">2019</span>,</span>
<span id="cb173-5"><a href="multiple-regression.html#cb173-5"></a>    <span class="dt">region =</span> <span class="st">&quot;tract:*&quot;</span>,</span>
<span id="cb173-6"><a href="multiple-regression.html#cb173-6"></a>    <span class="dt">regionin =</span> <span class="st">&quot;state:06+county:001,013,041,055,075,081,085,095,097&quot;</span>,</span>
<span id="cb173-7"><a href="multiple-regression.html#cb173-7"></a>    <span class="dt">vars =</span> <span class="kw">c</span>(</span>
<span id="cb173-8"><a href="multiple-regression.html#cb173-8"></a>      <span class="st">&quot;B06009_001E&quot;</span>,</span>
<span id="cb173-9"><a href="multiple-regression.html#cb173-9"></a>      <span class="st">&quot;B06009_002E&quot;</span>,</span>
<span id="cb173-10"><a href="multiple-regression.html#cb173-10"></a>      <span class="st">&quot;B06009_003E&quot;</span>,</span>
<span id="cb173-11"><a href="multiple-regression.html#cb173-11"></a>      <span class="st">&quot;B19001_001E&quot;</span>,</span>
<span id="cb173-12"><a href="multiple-regression.html#cb173-12"></a>      <span class="st">&quot;B19001_014E&quot;</span>,</span>
<span id="cb173-13"><a href="multiple-regression.html#cb173-13"></a>      <span class="st">&quot;B19001_015E&quot;</span>,</span>
<span id="cb173-14"><a href="multiple-regression.html#cb173-14"></a>      <span class="st">&quot;B19001_016E&quot;</span>,</span>
<span id="cb173-15"><a href="multiple-regression.html#cb173-15"></a>      <span class="st">&quot;B19001_017E&quot;</span>,</span>
<span id="cb173-16"><a href="multiple-regression.html#cb173-16"></a>      <span class="st">&quot;B19001A_001E&quot;</span></span>
<span id="cb173-17"><a href="multiple-regression.html#cb173-17"></a>    )</span>
<span id="cb173-18"><a href="multiple-regression.html#cb173-18"></a>  ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb173-19"><a href="multiple-regression.html#cb173-19"></a><span class="st">  </span><span class="kw">transmute</span>(</span>
<span id="cb173-20"><a href="multiple-regression.html#cb173-20"></a>    <span class="dt">tract =</span> <span class="kw">paste0</span>(state, county, tract),</span>
<span id="cb173-21"><a href="multiple-regression.html#cb173-21"></a>    <span class="dt">perc_college =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(B06009_002E <span class="op">+</span><span class="st"> </span>B06009_003E) <span class="op">/</span><span class="st"> </span>B06009_001E,</span>
<span id="cb173-22"><a href="multiple-regression.html#cb173-22"></a>    <span class="dt">perc_over100k =</span> (B19001_014E <span class="op">+</span><span class="st"> </span>B19001_015E <span class="op">+</span><span class="st"> </span>B19001_016E <span class="op">+</span><span class="st"> </span>B19001_017E) <span class="op">/</span><span class="st"> </span>B19001_001E,</span>
<span id="cb173-23"><a href="multiple-regression.html#cb173-23"></a>    <span class="dt">perc_white =</span> B19001A_001E <span class="op">/</span><span class="st"> </span>B19001_001E</span>
<span id="cb173-24"><a href="multiple-regression.html#cb173-24"></a>  ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb173-25"><a href="multiple-regression.html#cb173-25"></a><span class="st">  </span><span class="kw">filter</span>(</span>
<span id="cb173-26"><a href="multiple-regression.html#cb173-26"></a>    <span class="op">!</span><span class="kw">is.na</span>(perc_college), </span>
<span id="cb173-27"><a href="multiple-regression.html#cb173-27"></a>    <span class="op">!</span><span class="kw">is.na</span>(perc_over100k),</span>
<span id="cb173-28"><a href="multiple-regression.html#cb173-28"></a>    <span class="op">!</span><span class="kw">is.na</span>(perc_white)</span>
<span id="cb173-29"><a href="multiple-regression.html#cb173-29"></a>  )</span></code></pre></div>
<p>So at this point, the modified question could be: to what degree does information about BOTH race and educational attainment in a neighborhood tell us something about income? This can no longer be viewed on one scatter plot. One useful visualization tool that becomes useful once we are dealing with multiple variables is a correlation plot, which can be done in Excel, and in R is available through a number of packages, like <code>corrplot</code> (remember to install with <code>install.packages("corrplot")</code> if using for the first time):</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="multiple-regression.html#cb174-1"></a><span class="kw">library</span>(corrplot)</span>
<span id="cb174-2"><a href="multiple-regression.html#cb174-2"></a></span>
<span id="cb174-3"><a href="multiple-regression.html#cb174-3"></a>correlationplot &lt;-<span class="st"> </span>bay_multiple_tract <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb174-4"><a href="multiple-regression.html#cb174-4"></a><span class="st">  </span><span class="kw">select</span>(</span>
<span id="cb174-5"><a href="multiple-regression.html#cb174-5"></a>    perc_white, </span>
<span id="cb174-6"><a href="multiple-regression.html#cb174-6"></a>    perc_college,</span>
<span id="cb174-7"><a href="multiple-regression.html#cb174-7"></a>    perc_over100k</span>
<span id="cb174-8"><a href="multiple-regression.html#cb174-8"></a>  ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb174-9"><a href="multiple-regression.html#cb174-9"></a><span class="st">  </span><span class="kw">cor</span>()</span>
<span id="cb174-10"><a href="multiple-regression.html#cb174-10"></a></span>
<span id="cb174-11"><a href="multiple-regression.html#cb174-11"></a><span class="kw">corrplot</span>(</span>
<span id="cb174-12"><a href="multiple-regression.html#cb174-12"></a>  correlationplot, </span>
<span id="cb174-13"><a href="multiple-regression.html#cb174-13"></a>  <span class="dt">method =</span> <span class="st">&quot;number&quot;</span>,</span>
<span id="cb174-14"><a href="multiple-regression.html#cb174-14"></a>  <span class="dt">type =</span> <span class="st">&quot;upper&quot;</span></span>
<span id="cb174-15"><a href="multiple-regression.html#cb174-15"></a>)</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-185-1.png" width="672" /></p>
<p>There are many possible parameters to customize the visualization, but what I’ve shown above is the essential information. Note that the numbers are correlation coefficients, which are <code>R-squared</code> without the square, which you’ll recall from earlier this chapter is a measure of the amount of variance one variable shares with the other. The “1” values are self-evident as you are pairing a variable with itself. So the only other numbers are the results of the unique possible pairings. At this scale, we are treating any of these pairs as valid to evaluate, and agnostic about what we might ultimately treat as the “independent” vs. “dependent” variables (which is great to keep in mind as completely arbitrary, since we can make no claims of causality here). All of these numbers are positive, which is to say that increases in any one variable are associated with increases in the others (this was also an arbitrary consequence of the specific variables we designed; if we had switched <code>perc_white</code> with <code>perc_nonwhite</code>, we’d see some negative, red numbers in this plot). The correlation coefficient between <code>perc_college</code> and <code>perc_over100k</code> is 0.73; if we take the square of this, we get 0.53, which matches the <code>R-squared</code> from the first <code>lm()</code> output in the previous chapter. Note that the correlation coefficient between <code>perc_white</code> and <code>perc_college</code>, and <code>perc_white</code> and <code>perc_over100k</code>, are lower, which means we’d expect the scatter plots between these two pairs to have a looser fit around the regression line:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="multiple-regression.html#cb175-1"></a><span class="kw">ggplot</span>(</span>
<span id="cb175-2"><a href="multiple-regression.html#cb175-2"></a>  <span class="dt">data =</span> bay_multiple_tract,</span>
<span id="cb175-3"><a href="multiple-regression.html#cb175-3"></a>  <span class="kw">aes</span>(</span>
<span id="cb175-4"><a href="multiple-regression.html#cb175-4"></a>      <span class="dt">x =</span> perc_white,</span>
<span id="cb175-5"><a href="multiple-regression.html#cb175-5"></a>      <span class="dt">y =</span> perc_college</span>
<span id="cb175-6"><a href="multiple-regression.html#cb175-6"></a>    )</span>
<span id="cb175-7"><a href="multiple-regression.html#cb175-7"></a>) <span class="op">+</span></span>
<span id="cb175-8"><a href="multiple-regression.html#cb175-8"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb175-9"><a href="multiple-regression.html#cb175-9"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-186-1.png" width="672" /></p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="multiple-regression.html#cb176-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(perc_college <span class="op">~</span><span class="st"> </span>perc_white, bay_multiple_tract)</span>
<span id="cb176-2"><a href="multiple-regression.html#cb176-2"></a></span>
<span id="cb176-3"><a href="multiple-regression.html#cb176-3"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = perc_college ~ perc_white, data = bay_multiple_tract)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.48905 -0.10068  0.01654  0.10835  0.34437 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.521428   0.009783   53.30   &lt;2e-16 ***
## perc_white  0.366615   0.016006   22.91   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1438 on 1575 degrees of freedom
## Multiple R-squared:  0.2499, Adjusted R-squared:  0.2494 
## F-statistic: 524.6 on 1 and 1575 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="multiple-regression.html#cb178-1"></a><span class="kw">ggplot</span>(</span>
<span id="cb178-2"><a href="multiple-regression.html#cb178-2"></a>  <span class="dt">data =</span> bay_multiple_tract,</span>
<span id="cb178-3"><a href="multiple-regression.html#cb178-3"></a>  <span class="kw">aes</span>(</span>
<span id="cb178-4"><a href="multiple-regression.html#cb178-4"></a>      <span class="dt">x =</span> perc_white,</span>
<span id="cb178-5"><a href="multiple-regression.html#cb178-5"></a>      <span class="dt">y =</span> perc_over100k</span>
<span id="cb178-6"><a href="multiple-regression.html#cb178-6"></a>    )</span>
<span id="cb178-7"><a href="multiple-regression.html#cb178-7"></a>) <span class="op">+</span></span>
<span id="cb178-8"><a href="multiple-regression.html#cb178-8"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb178-9"><a href="multiple-regression.html#cb178-9"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-188-1.png" width="672" /></p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="multiple-regression.html#cb179-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(perc_over100k <span class="op">~</span><span class="st"> </span>perc_white, bay_multiple_tract)</span>
<span id="cb179-2"><a href="multiple-regression.html#cb179-2"></a></span>
<span id="cb179-3"><a href="multiple-regression.html#cb179-3"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = perc_over100k ~ perc_white, data = bay_multiple_tract)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.51235 -0.13376  0.01303  0.12491  0.47003 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.37999    0.01164   32.64   &lt;2e-16 ***
## perc_white   0.24022    0.01905   12.61   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1712 on 1575 degrees of freedom
## Multiple R-squared:  0.09171,    Adjusted R-squared:  0.09113 
## F-statistic:   159 on 1 and 1575 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Keep in mind that the slope of the line (what we might call the “effect size”) is a distinct concept from <code>R-squared</code> which is a measure of correlation. These don’t have to do anything with each other. What a high correlation coefficient and <code>R-squared</code> value tell you is that residual errors are low; what a steep slope tells you is that large changes in <code>x</code> are associated with large changes in <code>y</code>. In this case, education seems to have better predictive power (on income) compared to race, and it also predicts larger changes (in income) compared to race.</p>
<p>For ease of comparison with what follows, I’ll reproduce the scatter plot and <code>lm()</code> output for the original pairing of <code>perc_college</code> and <code>perc_over100k</code> from the previous chapter, so we have all three unique combinations of pairs:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="multiple-regression.html#cb181-1"></a><span class="kw">ggplot</span>(</span>
<span id="cb181-2"><a href="multiple-regression.html#cb181-2"></a>  <span class="dt">data =</span> bay_multiple_tract,</span>
<span id="cb181-3"><a href="multiple-regression.html#cb181-3"></a>  <span class="kw">aes</span>(</span>
<span id="cb181-4"><a href="multiple-regression.html#cb181-4"></a>      <span class="dt">x =</span> perc_college,</span>
<span id="cb181-5"><a href="multiple-regression.html#cb181-5"></a>      <span class="dt">y =</span> perc_over100k</span>
<span id="cb181-6"><a href="multiple-regression.html#cb181-6"></a>    )</span>
<span id="cb181-7"><a href="multiple-regression.html#cb181-7"></a>) <span class="op">+</span></span>
<span id="cb181-8"><a href="multiple-regression.html#cb181-8"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb181-9"><a href="multiple-regression.html#cb181-9"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-190-1.png" width="672" /></p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="multiple-regression.html#cb182-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(perc_over100k <span class="op">~</span><span class="st"> </span>perc_college, bay_multiple_tract)</span>
<span id="cb182-2"><a href="multiple-regression.html#cb182-2"></a></span>
<span id="cb182-3"><a href="multiple-regression.html#cb182-3"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = perc_over100k ~ perc_college, data = bay_multiple_tract)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.56483 -0.07131  0.01394  0.08257  0.37896 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.05996    0.01393  -4.305 1.77e-05 ***
## perc_college  0.78996    0.01862  42.435  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1227 on 1575 degrees of freedom
## Multiple R-squared:  0.5334, Adjusted R-squared:  0.5331 
## F-statistic:  1801 on 1 and 1575 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now let’s see the results of <code>lm()</code> with both race and education used as independent variables. All this entails is linking the two variables together with <code>+</code> on the right side of <code>~</code>:</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="multiple-regression.html#cb184-1"></a>full_model &lt;-<span class="st"> </span><span class="kw">lm</span>(perc_over100k <span class="op">~</span><span class="st"> </span>perc_college <span class="op">+</span><span class="st"> </span>perc_white, bay_multiple_tract)</span>
<span id="cb184-2"><a href="multiple-regression.html#cb184-2"></a></span>
<span id="cb184-3"><a href="multiple-regression.html#cb184-3"></a><span class="kw">summary</span>(full_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = perc_over100k ~ perc_college + perc_white, data = bay_multiple_tract)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.58219 -0.06857  0.01426  0.08389  0.40147 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.05532    0.01390  -3.980 7.21e-05 ***
## perc_college  0.83483    0.02138  39.045  &lt; 2e-16 ***
## perc_white   -0.06584    0.01568  -4.198 2.84e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.122 on 1574 degrees of freedom
## Multiple R-squared:  0.5386, Adjusted R-squared:  0.538 
## F-statistic: 918.7 on 2 and 1574 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Let’s inspect the (potentially surprising) results, in comparison with the previous three <code>lm()</code> outputs.</p>
<p>While the regression coefficient (which we’ve called “slope” previously, but since we can’t visualize this in a two-dimensional chart anymore, it’s better to reframe as just a coefficient you multiply <code>x</code> by) of <code>perc_white</code> was 0.24 when it alone was compared with <code>perc_over100k</code>. But now, alongside <code>perc_over100k</code>, its coefficient has effectively become 0. That is to say that, “controlling for education”, there appears to be a very small association (but still significant, per the asterisks) between race and income. Again, no causal claims here; perhaps there is actually a big causal mechanism here, but some other causal mechanism has an equal and opposite effect. We’re merely making claims about the shared variation in observations, and in this case, multiple observations.</p>
<p>Looking more closely: the regression coefficient for <code>perc_college</code> on its own was 0.79, but actually increased to 0.83 when combined with <code>perc_white</code>. So, we could say that, “controlling for race”, education appears even more associated with income. More specifically, the more white a census tract is, we would predict a (very slightly) lower number of high income households. But that would seem to go against what we saw earlier, where the scatter plot of <code>perc_white</code> vs. <code>perc_over100k</code> showed an overall positive relationship (effect size of 0.24). What’s going on here?</p>
<p>This happens to be a small example of a big issue in statistics, <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Simpson’s paradox</a>, which refers to situations where this kind of reversal appears to happen: positive association in one framing, negative association in another. What usually explains this paradox is a confounding variable, in this case education, such that within subgroups of education, White-ness has a muted, even negative association with income. To start to get a sense of this possible explanation, we can add one more “dimension” of color to a previous scatter plot <code>perc_white</code> vs. <code>perc_over100k</code>, so that “higher education” census tracts are lighter color than “lower education” census tracts.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="multiple-regression.html#cb186-1"></a><span class="kw">ggplot</span>(</span>
<span id="cb186-2"><a href="multiple-regression.html#cb186-2"></a>  <span class="dt">data =</span> bay_multiple_tract,</span>
<span id="cb186-3"><a href="multiple-regression.html#cb186-3"></a>  <span class="kw">aes</span>(</span>
<span id="cb186-4"><a href="multiple-regression.html#cb186-4"></a>      <span class="dt">x =</span> perc_white,</span>
<span id="cb186-5"><a href="multiple-regression.html#cb186-5"></a>      <span class="dt">y =</span> perc_over100k</span>
<span id="cb186-6"><a href="multiple-regression.html#cb186-6"></a>    )</span>
<span id="cb186-7"><a href="multiple-regression.html#cb186-7"></a>) <span class="op">+</span></span>
<span id="cb186-8"><a href="multiple-regression.html#cb186-8"></a><span class="st">  </span><span class="kw">geom_point</span>(</span>
<span id="cb186-9"><a href="multiple-regression.html#cb186-9"></a>    <span class="kw">aes</span>(</span>
<span id="cb186-10"><a href="multiple-regression.html#cb186-10"></a>      <span class="dt">color =</span> perc_college</span>
<span id="cb186-11"><a href="multiple-regression.html#cb186-11"></a>    )</span>
<span id="cb186-12"><a href="multiple-regression.html#cb186-12"></a>  )</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-193-1.png" width="672" /></p>
<p>We can roughly see that the colors are not completely randomly mixed; lighter colors tend to be to the upper right of the scatter plot. To visualize this even more clearly, we can directly break this scatter plot into a few smaller scatter plots with different amounts of education, each with their own trendlines. To help us do this, we are going to use a function called <code>quantcut()</code>, which lets you convert a vector of continuous data into a discrete number of factor categories; just think of this as a shortcut to using <code>case_when()</code> to manually assign <code>perc_college</code> values in different tiers to string descriptions of the tiers. <code>quantcut()</code> comes from a package called <code>gtools</code>, so make sure to install and load that.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="multiple-regression.html#cb187-1"></a><span class="kw">library</span>(gtools)</span>
<span id="cb187-2"><a href="multiple-regression.html#cb187-2"></a></span>
<span id="cb187-3"><a href="multiple-regression.html#cb187-3"></a>bay_multiple_tract <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb187-4"><a href="multiple-regression.html#cb187-4"></a><span class="st">  </span><span class="kw">mutate</span>(</span>
<span id="cb187-5"><a href="multiple-regression.html#cb187-5"></a>    <span class="dt">college_round =</span> <span class="kw">quantcut</span>(perc_college, <span class="dv">4</span>)</span>
<span id="cb187-6"><a href="multiple-regression.html#cb187-6"></a>  ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb187-7"><a href="multiple-regression.html#cb187-7"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb187-8"><a href="multiple-regression.html#cb187-8"></a><span class="st">  </span><span class="kw">geom_point</span>(</span>
<span id="cb187-9"><a href="multiple-regression.html#cb187-9"></a>    <span class="kw">aes</span>(</span>
<span id="cb187-10"><a href="multiple-regression.html#cb187-10"></a>        <span class="dt">x =</span> perc_white,</span>
<span id="cb187-11"><a href="multiple-regression.html#cb187-11"></a>        <span class="dt">y =</span> perc_over100k,</span>
<span id="cb187-12"><a href="multiple-regression.html#cb187-12"></a>        <span class="dt">color =</span> college_round</span>
<span id="cb187-13"><a href="multiple-regression.html#cb187-13"></a>      )</span>
<span id="cb187-14"><a href="multiple-regression.html#cb187-14"></a>  ) <span class="op">+</span></span>
<span id="cb187-15"><a href="multiple-regression.html#cb187-15"></a><span class="st">  </span><span class="kw">geom_smooth</span>(</span>
<span id="cb187-16"><a href="multiple-regression.html#cb187-16"></a>    <span class="kw">aes</span>(</span>
<span id="cb187-17"><a href="multiple-regression.html#cb187-17"></a>        <span class="dt">x =</span> perc_white,</span>
<span id="cb187-18"><a href="multiple-regression.html#cb187-18"></a>        <span class="dt">y =</span> perc_over100k,</span>
<span id="cb187-19"><a href="multiple-regression.html#cb187-19"></a>        <span class="dt">color =</span> college_round</span>
<span id="cb187-20"><a href="multiple-regression.html#cb187-20"></a>      ),</span>
<span id="cb187-21"><a href="multiple-regression.html#cb187-21"></a>    <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb187-22"><a href="multiple-regression.html#cb187-22"></a>    <span class="dt">se =</span> F</span>
<span id="cb187-23"><a href="multiple-regression.html#cb187-23"></a>  ) <span class="op">+</span></span>
<span id="cb187-24"><a href="multiple-regression.html#cb187-24"></a><span class="st">  </span><span class="kw">geom_smooth</span>(</span>
<span id="cb187-25"><a href="multiple-regression.html#cb187-25"></a>    <span class="kw">aes</span>(</span>
<span id="cb187-26"><a href="multiple-regression.html#cb187-26"></a>      <span class="dt">x =</span> perc_white,</span>
<span id="cb187-27"><a href="multiple-regression.html#cb187-27"></a>      <span class="dt">y =</span> perc_over100k</span>
<span id="cb187-28"><a href="multiple-regression.html#cb187-28"></a>    ),</span>
<span id="cb187-29"><a href="multiple-regression.html#cb187-29"></a>    <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb187-30"><a href="multiple-regression.html#cb187-30"></a>    <span class="dt">se =</span> F,</span>
<span id="cb187-31"><a href="multiple-regression.html#cb187-31"></a>    <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb187-32"><a href="multiple-regression.html#cb187-32"></a>    <span class="dt">linetype =</span> <span class="dv">2</span></span>
<span id="cb187-33"><a href="multiple-regression.html#cb187-33"></a>  )</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-194-1.png" width="672" /></p>
<p>The dashed black line is the original best-fit line we’ve seen before, but in addition, we’ve separated the data into quartiles of education (i.e., red is the lowest 25% of census tracts by <code>perc_college</code>, and purple is the highest 25%), each with separate best-fit lines. While the best-fit line for the lowest educational attainment census tracts still appears to be positive slope (i.e., more White-ness predicts more income), for the rest of the census tracts, White-ness has a slightly negative association with income.</p>
<p>Here’s one more way to better visualize this confounding relationship. If we have run <code>lm()</code> on <code>perc_college</code> vs. <code>perc_over100k</code>, then, recall from the the previous chapter, <code>residuals(model)</code> holds all the “residuals”, or “remaining unexplained variation in <code>perc_over100k</code> after <code>perc_college</code> has been accounted for”. So if we were to plot <code>perc_white</code> vs. these residuals, we’d see the following:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="multiple-regression.html#cb188-1"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(perc_over100k <span class="op">~</span><span class="st"> </span>perc_college, bay_multiple_tract)</span>
<span id="cb188-2"><a href="multiple-regression.html#cb188-2"></a></span>
<span id="cb188-3"><a href="multiple-regression.html#cb188-3"></a><span class="kw">ggplot</span>(</span>
<span id="cb188-4"><a href="multiple-regression.html#cb188-4"></a>  <span class="dt">data =</span> bay_multiple_tract,</span>
<span id="cb188-5"><a href="multiple-regression.html#cb188-5"></a>  <span class="kw">aes</span>(</span>
<span id="cb188-6"><a href="multiple-regression.html#cb188-6"></a>      <span class="dt">x =</span> perc_white,</span>
<span id="cb188-7"><a href="multiple-regression.html#cb188-7"></a>      <span class="dt">y =</span> model<span class="op">$</span>residuals</span>
<span id="cb188-8"><a href="multiple-regression.html#cb188-8"></a>    )</span>
<span id="cb188-9"><a href="multiple-regression.html#cb188-9"></a>) <span class="op">+</span></span>
<span id="cb188-10"><a href="multiple-regression.html#cb188-10"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb188-11"><a href="multiple-regression.html#cb188-11"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<p><img src="course_files/figure-html/unnamed-chunk-195-1.png" width="672" /></p>
<p>This is not exactly the same negative regression coefficient for <code>perc_white</code> as what we see in the multiple regression results, but tells the same story: once the explanatory power of education has been accounted for, the census tracts where education tends to “underestimate” income (i.e., the residuals are positive) tend to have fewer White-led households (top and left in the plot), while the census tracts where education tends to “overestimate” income (i.e., the residuals are negative) tend have more White-led households.</p>
<p>Notice how this more nuanced interaction between race and education was “invisible” to us before we started to dig deeper into multiple regression, which goes to show how complex statistical analysis can be. I would recommend you always carefully consider the possibility of lurking variables, using a combination of this visual scatter plot technique and the results of the multiple regression itself.</p>
<p>Returning to our examination of the results of the multiple regression: <code>R-squared</code> for <code>perc_white</code> vs. <code>perc_over100k</code> was 0.092, and <code>R-squared</code> for <code>perc_collegedegree</code> vs. <code>perc_over100k</code> was 0.533. <code>R-squared</code> for both independent variables combined was 0.539, only slightly higher. You can ONLY maintain or increase <code>R-squared</code> (which is to say, reduce residuals) when adding new variables, because at its worst, a new independent variable can simply have a regression coefficient of 0 and have no effect size. Essentially, <code>perc_white</code> has been demoted to almost this status, which is to say that whatever explanatory power race had is virtually entirely coinciding with the explanatory power of education (an analogy would be that a sundial tells time, and a wristwatch tells time better, and if you have both, a sundial is virtually useless). The other clue we had of this result was the other correlation coefficient from the correlation plot, which was 0.50 between the two independent variables. If your independent variables are correlated, then they’ll share some amount of explanatory power (which is why <code>R-squared</code> is usually less than the sum of its <code>R-squared</code> parts).</p>
<p>This kind of evaluation is worth doing on any combination of independent variables. You might wonder: if I can just keep adding independent variables and get higher and higher <code>R-squared</code>, why don’t I just use every single possible variable I can get from the ACS? And you would have re-invented machine learning, which does essentially that (often described as “throwing everything in the kitchen sink”). The benefit is predictive capability (i.e. reduction of residual errors), but the cost is interpretability of the regression coefficients (“slopes”) of individual variables, which can go haywire from a mathematical perspective if you include too many similar variables. One last useful check to do on your multiple regression models is the <a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">variance inflation factor</a> (VIF), which is a formal mathematical measure of how much variables in your set of multiple variables may be collinear (highly correlated) with each other (which can cause the shared size of their explanatory power to look distorted in your results). We can use <code>vif()</code> from the <code>car</code> package:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="multiple-regression.html#cb189-1"></a><span class="kw">library</span>(car)</span>
<span id="cb189-2"><a href="multiple-regression.html#cb189-2"></a><span class="kw">vif</span>(full_model)</span></code></pre></div>
<pre><code>## perc_college   perc_white 
##     1.333099     1.333099</code></pre>
<p>A VIF value of 1 indicates no collinearity; VIF values above 5 suggest a level of collinearity that has a problematic influence on model interpretation (<a href="https://link.springer.com/book/10.1007%2F978-1-4614-7138-7">James et al. 2013</a>), in which case you should generally remove the variable with this highest VIF value and re-run your model.</p>
<p>Ultimately, for non-machine-learning applications, where you want the story of the regression analysis to be meaningful to stakeholders: the goal should be to have just a handful of independent variables, each as uncorrelated with each other as possible, which is to say they likely represent distinct dimensions of characteristics about a population that all seem to “explain” something about the outcome. As you add each additional independent variable to the multiple regression, look for non-negligible change in <code>R-squared</code>; if there is no change, that variable is highly correlated with existing variables and/or has no explanatory power, and therefore is probably not worth adding. There are advanced techniques (like “stepwise regression”) you can pick up to help with the search process for good independent variables, but often intuition and literature review are your best tools.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="autocorrelation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": false,
"toc": {
"collapse": "none",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": false
});
});
</script>

</body>

</html>
